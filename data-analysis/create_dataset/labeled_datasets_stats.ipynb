{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the C -> A dataset to be used by the fine-tuned BERT classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and thoughts\n",
    "- Tutorial: https://huggingface.co/docs/transformers/custom_datasets\n",
    "- Context texts must be limited to 512 tokens (Limit for BERT model)\n",
    "- When labeling the dataset, should the labels be start, end, or start and inside? In other projects (with answer extraction) it seems they use start, end..\n",
    "- Another option is to insert a higlight token around the sentence containing the answer, and then append the answers after a [SEP] token. As in: \n",
    "- There are multiple answer spans in the same context text.. Should those be labeled jointly? / should I have multiple instances of the same texts?\n",
    "- My idea is to use the original text, no stopword removal or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports, to be combined into the final datastructure\n",
    "CA_df = pd.read_pickle(\"./data/CA/labeled_CA_data_train.pkl\")\n",
    "CA_df_eval = pd.read_pickle(\"./data/CA/labeled_CA_data_eval.pkl\")\n",
    "CA_df_test = pd.read_pickle(\"./data/CA/labeled_CA_data_test.pkl\")\n",
    "\n",
    "# CAR_df = pd.read_pickle(\"./data/CAR/labeled_CAR_data_train.pkl\")\n",
    "\n",
    "CAR_sent_class_df = pd.read_pickle(\"./data/CAR_classification/labeled_CAR_data_train.pkl\")\n",
    "CAR_sent_class_df_eval = pd.read_pickle(\"./data/CAR_classification/labeled_CAR_data_eval.pkl\")\n",
    "CAR_sent_class_df_test = pd.read_pickle(\"./data/CAR_classification/labeled_CAR_data_test.pkl\")\n",
    "\n",
    "CRA_df = pd.read_pickle(\"./data/CRA/labeled_CRA_data_train.pkl\")\n",
    "CRA_df_eval = pd.read_pickle(\"./data/CRA/labeled_CRA_data_eval.pkl\")\n",
    "CRA_df_test = pd.read_pickle(\"./data/CRA/labeled_CRA_data_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the class weights to use in the training of the C -> A model (to account for the scarse dataset)\n",
    "# idea for how to scale weights:\n",
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#calculate_class_weights\n",
    "# https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4\n",
    "\n",
    "def get_class_distribution(labeled_df, is_sent_class):\n",
    "    if is_sent_class:\n",
    "        nr_classes = 2\n",
    "    else:\n",
    "        nr_classes = 3\n",
    "    counts = np.zeros(nr_classes)\n",
    "    for idx, point in labeled_df.iterrows():\n",
    "        if 'labels' in point.keys():\n",
    "            labels = point['labels']\n",
    "            for label in labels:\n",
    "                if int(label) >= 0:\n",
    "                    counts[int(label)] += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            label = point['label']\n",
    "            if int(label) >= 0:\n",
    "                counts[int(label)] += 1\n",
    "\n",
    "    num_labels = np.sum(counts)\n",
    "    ins_weights_raw = 1 / counts\n",
    "    ins_weights = ins_weights_raw * (num_labels/2)\n",
    "    ins_weights_norm = ins_weights_raw / np.sum(ins_weights_raw) * nr_classes\n",
    "\n",
    "    isns_weights_raw = 1 / np.sqrt(counts)\n",
    "    isns_weights = isns_weights_raw * (math.sqrt(num_labels/2))\n",
    "    isns_weights_norm = isns_weights_raw / np.sum(isns_weights_raw) * nr_classes\n",
    "\n",
    "    # ENS\n",
    "    B = 0.99999\n",
    "    E_nc = (1.0 - np.power(B, counts)) / (1.0 - B)\n",
    "    w = 1/E_nc\n",
    "    # normalize:\n",
    "    w = w / np.sum(w) * nr_classes\n",
    "\n",
    "    # norm = np.linalg.norm(weights)\n",
    "    # normal_array = weights/norm\n",
    "    print('fraction of each label: ', counts/num_labels)\n",
    "    print('INS: ',ins_weights)\n",
    "    print('INS, norm: ',ins_weights_norm)\n",
    "    print('ISNS: ',isns_weights)\n",
    "    print('ISNS, norm: ',isns_weights_norm)\n",
    "    print('ENS: ',w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  557\n",
      "number of train data points:  453\n",
      "number of eval data points:  104\n",
      "number of test data points:  41\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.9733263  0.00662795 0.02004575]\n",
      "INS:  [ 0.51370234 75.43814027 24.94293903]\n",
      "INS, norm:  [0.0152744 2.2430736 0.741652 ]\n",
      "ISNS:  [0.71673031 8.68551324 4.99429064]\n",
      "ISNS, norm:  [0.14935476 1.80991754 1.0407277 ]\n",
      "ENS:  [0.03317115 2.22251493 0.74431392]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.97339812 0.00665904 0.01994284]\n",
      "INS:  [ 0.51366444 75.08590734 25.07165879]\n",
      "INS, norm:  [0.01530719 2.23755805 0.74713477]\n",
      "ISNS:  [0.71670387 8.66521248 5.00716075]\n",
      "ISNS, norm:  [0.14942665 1.80662299 1.04395036]\n",
      "ENS:  [0.0380304  2.21191891 0.75005068]\n"
     ]
    }
   ],
   "source": [
    "CA_all = pd.concat([CA_df, CA_df_eval])\n",
    "print('number of data points: ', len(CA_all))\n",
    "print('number of train data points: ', len(CA_df))\n",
    "print('number of eval data points: ', len(CA_df_eval))\n",
    "print('number of test data points: ', len(CA_df_test))\n",
    "CA_all = pd.concat([CA_df, CA_df_eval])\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CA_df, False)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CA_all, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_class_distribution(CAR_df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  1734\n",
      "number of train data points:  1419\n",
      "number of eval data points:  315\n",
      "number of test data points:  80\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.9928871  0.00178927 0.00532363]\n",
      "INS:  [  0.50358193 279.44283698  93.92089658]\n",
      "INS, norm:  [0.00404086 2.24231559 0.75364355]\n",
      "ISNS:  [ 0.70963507 16.71654381  9.69127941]\n",
      "ISNS, norm:  [0.07850681 1.84934852 1.07214466]\n",
      "ENS:  [0.03116612 2.21420097 0.7546329 ]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.99287172 0.00180052 0.00532776]\n",
      "INS:  [  0.50358973 277.6971709   93.84809756]\n",
      "INS, norm:  [0.00406067 2.23919922 0.75674011]\n",
      "ISNS:  [ 0.70964056 16.66424828  9.68752278]\n",
      "ISNS, norm:  [0.07867002 1.84738127 1.07394872]\n",
      "ENS:  [0.03785552 2.20446832 0.75767616]\n"
     ]
    }
   ],
   "source": [
    "CRA_all = pd.concat([CRA_df, CRA_df_eval])\n",
    "print('number of data points: ', len(CRA_all))\n",
    "print('number of train data points: ', len(CRA_df))\n",
    "print('number of eval data points: ', len(CRA_df_eval))\n",
    "print('number of test data points: ', len(CRA_df_test))\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CRA_df, False)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CRA_all, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  54214\n",
      "number of train data points:  45073\n",
      "number of eval data points:  9141\n",
      "number of test data points:  1894\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.91458301 0.08541699]\n",
      "INS:  [0.54669723 5.85363636]\n",
      "INS, norm:  [0.17083398 1.82916602]\n",
      "ISNS:  [0.73938977 2.41942893]\n",
      "ISNS, norm:  [0.46814321 1.53185679]\n",
      "ENS:  [0.20111125 1.79888875]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.91458301 0.08541699]\n",
      "INS:  [0.54669723 5.85363636]\n",
      "INS, norm:  [0.17083398 1.82916602]\n",
      "ISNS:  [0.73938977 2.41942893]\n",
      "ISNS, norm:  [0.46814321 1.53185679]\n",
      "ENS:  [0.20111125 1.79888875]\n"
     ]
    }
   ],
   "source": [
    "CAR_class_all = pd.concat([CAR_sent_class_df, CAR_sent_class_df_eval])\n",
    "print('number of data points: ', len(CAR_class_all))\n",
    "print('number of train data points: ', len(CAR_sent_class_df))\n",
    "print('number of eval data points: ', len(CAR_sent_class_df_eval))\n",
    "print('number of test data points: ', len(CAR_sent_class_df_test))\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CAR_sent_class_df, True)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CAR_sent_class_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of each label:  [0.925 0.075]\n",
      "INS:  [0.54054054 6.66666667]\n",
      "INS, norm:  [0.15 1.85]\n",
      "ISNS:  [0.73521462 2.5819889 ]\n",
      "ISNS, norm:  [0.44327375 1.55672625]\n",
      "ENS:  [0.15236789 1.84763211]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution of CAR class data subset\n",
    "train_path = \"../../model/data/CAR_classification/data_subset/CAR_class_data_train.pkl\"\n",
    "val_path = \"../../model/data/CAR_classification/data_subset/CAR_class_data_eval.pkl\"\n",
    "with open(train_path, \"rb\") as input_file:\n",
    "        train_data = pickle.load(input_file)\n",
    "with open(val_path, \"rb\") as input_file:\n",
    "    val_data = pickle.load(input_file)\n",
    "\n",
    "CAR_sub_train = pd.DataFrame(train_data)\n",
    "CAR_sub_eval = pd.DataFrame(train_data)\n",
    "CAR_sub_all = pd.concat([CAR_sub_train, CAR_sub_eval])\n",
    "get_class_distribution(CAR_sub_all, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de4ab37d9aa598fa28430b4c5abb54602406a240d03eddec7af88b85de3986f7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
