{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the C -> A dataset to be used by the fine-tuned BERT classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and thoughts\n",
    "- Tutorial: https://huggingface.co/docs/transformers/custom_datasets\n",
    "- Context texts must be limited to 512 tokens (Limit for BERT model)\n",
    "- When labeling the dataset, should the labels be start, end, or start and inside? In other projects (with answer extraction) it seems they use start, end..\n",
    "- Another option is to insert a higlight token around the sentence containing the answer, and then append the answers after a [SEP] token. As in: \n",
    "- There are multiple answer spans in the same context text.. Should those be labeled jointly? / should I have multiple instances of the same texts?\n",
    "- My idea is to use the original text, no stopword removal or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports, to be combined into the final datastructure\n",
    "CA_df = pd.read_pickle(\"./data/CA/labeled_CA_data_train.pkl\")\n",
    "CA_df_eval = pd.read_pickle(\"./data/CA/labeled_CA_data_eval.pkl\")\n",
    "CAR_df = pd.read_pickle(\"./data/CAR/labeled_CAR_data_train.pkl\")\n",
    "CAR_sent_class_df = pd.read_pickle(\"./data/CAR_classification/labeled_CAR_data_train.pkl\")\n",
    "CAR_sent_class_df_eval = pd.read_pickle(\"./data/CAR_classification/labeled_CAR_data_eval.pkl\")\n",
    "CRA_df = pd.read_pickle(\"./data/CRA/labeled_CRA_data_train.pkl\")\n",
    "CRA_df_eval = pd.read_pickle(\"./data/CRA/labeled_CRA_data_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the class weights to use in the training of the C -> A model (to account for the scarse dataset)\n",
    "# idea for how to scale weights:\n",
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#calculate_class_weights\n",
    "# https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4\n",
    "\n",
    "def get_class_distribution(labeled_df, is_sent_class):\n",
    "    if is_sent_class:\n",
    "        nr_classes = 2\n",
    "    else:\n",
    "        nr_classes = 3\n",
    "    counts = np.zeros(nr_classes)\n",
    "    for idx, point in labeled_df.iterrows():\n",
    "        if 'labels' in point.keys():\n",
    "            labels = point['labels']\n",
    "            for label in labels:\n",
    "                if int(label) >= 0:\n",
    "                    counts[int(label)] += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            label = point['label']\n",
    "            if int(label) >= 0:\n",
    "                counts[int(label)] += 1\n",
    "\n",
    "    num_labels = np.sum(counts)\n",
    "    ins_weights_raw = 1 / counts\n",
    "    ins_weights = ins_weights_raw * (num_labels/2)\n",
    "    ins_weights_norm = ins_weights_raw / np.sum(ins_weights_raw) * nr_classes\n",
    "\n",
    "    isns_weights_raw = 1 / np.sqrt(counts)\n",
    "    isns_weights = isns_weights_raw * (math.sqrt(num_labels/2))\n",
    "    isns_weights_norm = isns_weights_raw / np.sum(isns_weights_raw) * nr_classes\n",
    "\n",
    "    # ENS\n",
    "    B = 0.99999\n",
    "    E_nc = (1.0 - np.power(B, counts)) / (1.0 - B)\n",
    "    w = 1/E_nc\n",
    "    # normalize:\n",
    "    w = w / np.sum(w) * nr_classes\n",
    "\n",
    "    # norm = np.linalg.norm(weights)\n",
    "    # normal_array = weights/norm\n",
    "    print('fraction of each label: ', counts/num_labels)\n",
    "    print('INS: ',ins_weights)\n",
    "    print('INS, norm: ',ins_weights_norm)\n",
    "    print('ISNS: ',isns_weights)\n",
    "    print('ISNS, norm: ',isns_weights_norm)\n",
    "    print('ENS: ',w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  498\n",
      "number of train data points:  400\n",
      "number of eval data points:  98\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.97245636 0.00689324 0.0206504 ]\n",
      "INS:  [ 0.51416189 72.53486395 24.2126029 ]\n",
      "INS, norm:  [0.01585914 2.23731182 0.74682904]\n",
      "ISNS:  [0.71705083 8.51674022 4.92063034]\n",
      "ISNS, norm:  [0.15197742 1.80510527 1.04291731]\n",
      "ENS:  [0.03203581 2.21865512 0.74930907]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.97257313 0.00690304 0.02052383]\n",
      "INS:  [ 0.51410016 72.43186511 24.3619213 ]\n",
      "INS, norm:  [0.0158497  2.23307281 0.7510775 ]\n",
      "ISNS:  [0.71700778 8.51069122 4.93577971]\n",
      "ISNS, norm:  [0.15187112 1.80266968 1.0454592 ]\n",
      "ENS:  [0.03659775 2.20955627 0.75384598]\n"
     ]
    }
   ],
   "source": [
    "CA_all = pd.concat([CA_df, CA_df_eval])\n",
    "print('number of data points: ', len(CA_all))\n",
    "print('number of train data points: ', len(CA_df))\n",
    "print('number of eval data points: ', len(CA_df_eval))\n",
    "CA_all = pd.concat([CA_df, CA_df_eval])\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CA_df, False)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CA_all, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of each label:  [0.91798933 0.00454289 0.07746777]\n",
      "INS:  [  0.54466864 110.06202837   6.45429684]\n",
      "INS, norm:  [0.01395859 2.82063285 0.16540856]\n",
      "ISNS:  [ 0.73801669 10.49104515  2.54053082]\n",
      "ISNS, norm:  [0.16079271 2.28569837 0.55350893]\n",
      "ENS:  [0.08833199 2.70726576 0.20440225]\n"
     ]
    }
   ],
   "source": [
    "get_class_distribution(CAR_df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  1569\n",
      "number of train data points:  1274\n",
      "number of eval data points:  295\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.99317432 0.0017285  0.00509718]\n",
      "INS:  [  0.5034363  289.26768868  98.09344175]\n",
      "INS, norm:  [0.00389391 2.23738681 0.75871928]\n",
      "ISNS:  [ 0.70953245 17.00787137  9.90421333]\n",
      "ISNS, norm:  [0.07706273 1.84723486 1.0757024 ]\n",
      "ENS:  [0.02798288 2.21243967 0.75957744]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.99315231 0.00174335 0.00510434]\n",
      "INS:  [  0.50344745 286.80376516  97.95586312]\n",
      "INS, norm:  [0.00392029 2.23330848 0.76277123]\n",
      "ISNS:  [ 0.70954031 16.93528167  9.89726544]\n",
      "ISNS, norm:  [0.07728611 1.84466211 1.07805178]\n",
      "ENS:  [0.03424446 2.20221551 0.76354003]\n"
     ]
    }
   ],
   "source": [
    "CRA_all = pd.concat([CRA_df, CRA_df_eval])\n",
    "print('number of data points: ', len(CRA_all))\n",
    "print('number of train data points: ', len(CRA_df))\n",
    "print('number of eval data points: ', len(CRA_df_eval))\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CRA_df, False)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CRA_all, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  50273\n",
      "number of train data points:  41554\n",
      "number of eval data points:  8719\n",
      "Distribution training data: \n",
      "fraction of each label:  [0.92027242 0.07972758]\n",
      "INS:  [0.54331738 6.27135527]\n",
      "INS, norm:  [0.15945517 1.84054483]\n",
      "ISNS:  [0.73710066 2.50426741]\n",
      "ISNS, norm:  [0.45480837 1.54519163]\n",
      "ENS:  [0.18601511 1.81398489]\n",
      "Distribution training and eval data: \n",
      "fraction of each label:  [0.92027242 0.07972758]\n",
      "INS:  [0.54331738 6.27135527]\n",
      "INS, norm:  [0.15945517 1.84054483]\n",
      "ISNS:  [0.73710066 2.50426741]\n",
      "ISNS, norm:  [0.45480837 1.54519163]\n",
      "ENS:  [0.18601511 1.81398489]\n"
     ]
    }
   ],
   "source": [
    "CAR_class_all = pd.concat([CAR_sent_class_df, CAR_sent_class_df_eval])\n",
    "print('number of data points: ', len(CAR_class_all))\n",
    "print('number of train data points: ', len(CAR_sent_class_df))\n",
    "print('number of eval data points: ', len(CAR_sent_class_df_eval))\n",
    "print('Distribution training data: ')\n",
    "get_class_distribution(CAR_sent_class_df, True)\n",
    "print('Distribution training and eval data: ')\n",
    "get_class_distribution(CAR_sent_class_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of each label:  [0.925 0.075]\n",
      "INS:  [0.54054054 6.66666667]\n",
      "INS, norm:  [0.15 1.85]\n",
      "ISNS:  [0.73521462 2.5819889 ]\n",
      "ISNS, norm:  [0.44327375 1.55672625]\n",
      "ENS:  [0.15236789 1.84763211]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution of CAR class data subset\n",
    "train_path = \"../../model/data/CAR_classification/data_subset/CAR_class_data_train.pkl\"\n",
    "val_path = \"../../model/data/CAR_classification/data_subset/CAR_class_data_eval.pkl\"\n",
    "with open(train_path, \"rb\") as input_file:\n",
    "        train_data = pickle.load(input_file)\n",
    "with open(val_path, \"rb\") as input_file:\n",
    "    val_data = pickle.load(input_file)\n",
    "\n",
    "CAR_sub_train = pd.DataFrame(train_data)\n",
    "CAR_sub_eval = pd.DataFrame(train_data)\n",
    "CAR_sub_all = pd.concat([CAR_sub_train, CAR_sub_eval])\n",
    "get_class_distribution(CAR_sub_all, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de4ab37d9aa598fa28430b4c5abb54602406a240d03eddec7af88b85de3986f7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
