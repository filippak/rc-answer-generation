{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 4.57MB/s]                    \n",
      "2022-02-21 13:31:28 INFO: Downloading these customized packages for language: sv (Swedish)...\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "| depparse  | talbanken |\n",
      "| pretrain  | talbanken |\n",
      "=========================\n",
      "\n",
      "2022-02-21 13:31:28 INFO: File exists: /Users/filippakarrfelt/stanza_resources/sv/tokenize/talbanken.pt.\n",
      "2022-02-21 13:31:28 INFO: File exists: /Users/filippakarrfelt/stanza_resources/sv/pos/talbanken.pt.\n",
      "2022-02-21 13:31:28 INFO: File exists: /Users/filippakarrfelt/stanza_resources/sv/lemma/talbanken.pt.\n",
      "2022-02-21 13:31:28 INFO: File exists: /Users/filippakarrfelt/stanza_resources/sv/depparse/talbanken.pt.\n",
      "2022-02-21 13:31:28 INFO: File exists: /Users/filippakarrfelt/stanza_resources/sv/pretrain/talbanken.pt.\n",
      "2022-02-21 13:31:28 INFO: Finished downloading models and saved to /Users/filippakarrfelt/stanza_resources.\n",
      "2022-02-21 13:31:28 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "| depparse  | talbanken |\n",
      "=========================\n",
      "\n",
      "2022-02-21 13:31:28 INFO: Use device: cpu\n",
      "2022-02-21 13:31:28 INFO: Loading: tokenize\n",
      "2022-02-21 13:31:29 INFO: Loading: pos\n",
      "2022-02-21 13:31:29 INFO: Loading: lemma\n",
      "2022-02-21 13:31:29 INFO: Loading: depparse\n",
      "2022-02-21 13:31:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# use the stanza tokenizer\n",
    "stanza.download('sv', processors='tokenize,pos,lemma,depparse')\n",
    "nlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_train = pd.read_json(r'../data/training.json', orient='split')\n",
    "df_train.head()\n",
    "context_corpus = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_corpus(doc):\n",
    "    for sentence in doc.sentences:\n",
    "        for raw_word in sentence.words:\n",
    "            # only add if character is letter or number (removes , . ? ! etc.)\n",
    "            w_r  = re.sub('[^\\s]', '', raw_word.text) # only remove space, escape char etc.\n",
    "            if not w_r.isnumeric():\n",
    "                context_corpus.add(w_r.lower())\n",
    "            w_1  = re.sub('[^\\sa-zåäöA-ZÅÄÖ0-9_-]', '', raw_word.text) # braod definition of words, including numbers, _ and -\n",
    "            w_2  = re.sub('[^\\sa-zåäöA-ZÅÄÖ]', '', raw_word.text)\n",
    "            if len(w_1) > 0 and not w_1.isnumeric():\n",
    "                context_corpus.add(w_1.lower())\n",
    "            if len(w_2) > 0:\n",
    "                context_corpus.add(w_2.lower())\n",
    "            word_lemma = str(raw_word.lemma)\n",
    "            if word_lemma != raw_word.text and not word_lemma.isnumeric():\n",
    "                context_corpus.add(word_lemma.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_words(df):\n",
    "    print('adding context words to corpus..')\n",
    "    for index, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        context_parsed = nlp(context)\n",
    "        add_words_to_corpus(context_parsed)\n",
    "    return context_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding context words to corpus..\n"
     ]
    }
   ],
   "source": [
    "# Re-generate the context corpus\n",
    "context_corpus = add_context_words(df_train)\n",
    "context_corpus_list = list(context_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save context corpus to file\n",
    "def save_context_corpus(filename, list):\n",
    "    list.sort()\n",
    "    with open(filename, 'w') as out:\n",
    "        for word in list:\n",
    "            out.write(word + '\\n')\n",
    "\n",
    "save_context_corpus('../context-corpus.txt', context_corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de4ab37d9aa598fa28430b4c5abb54602406a240d03eddec7af88b85de3986f7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('dp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
