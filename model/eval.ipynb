{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Laborator', '##ie', '##ingenjör', 'Laborator', '##ie', '##ingenjörer', 'kan', 'bland', 'annat', 'arbeta', 'inom', 'den', 'kemiska', 'industrin', ',', 'massa', '-', 'och', 'pappers', '##industrin', 'eller', 'läkemedels', '##industrin', ',', 'men', 'också', 'på', 'forsknings', '##institutioner', 'vid', 'högskolor', 'och', 'universitet', '.', 'Det', 'finns', 'även', 'laboratorie', '##ingenjörer', 'vid', 'sjukhus', 'och', 'myndigheter', 'som', 'Livsmedel', '##sverket', 'och', 'Statens', 'Kriminal', '##tekniska', 'Laborator', '##ium', '.', 'Arbetsuppgifter', 'På', 'laboratoriet', 'arbetar', 'laboratorie', '##ingenjören', 'med', 'tekniskt', 'avancerade', 'analys', '##apparater', 'och', 'dator', '##teknik', 'för', 'att', 'göra', 'analyser', 'och', 'kontroller', '.', 'Man', 'kan', 'undersöka', 'råvaror', 'som', 'används', 'vid', 'tillverkningen', 'på', 'en', 'industri', 'eller', 'prover', 'som', 'tagits', 'ute', 'i', 'produktionen', 'för', 'att', 'kontrollera', 'att', 'produkten', 'håller', 'rätt', 'kvalitet', 'och', 'att', 'tillverknings', '##processen', 'fungerar', 'som', 'den', 'ska', '.', 'Andra', 'analyser', 'är', 'kemiska', 'och', 'biologiska', 'tester', 'för', 'att', 'undersöka', 'vad', 'som', 'finns', 'i', 'produkter', '.', 'Det', 'kan', 'till', 'exempel', 'handla', 'om', 'att', 'leta', 'efter', 'ämnen', 'som', 'är', 'skadliga', 'för', 'miljön', 'eller', 'mikro', '##organis', '##mer', 'som', 'kan', 'göra', 'oss', 'sjuka', '.', 'När', 'en', 'analys', 'är', 'avslutad', 'tolkas', 'svaren', 'och', 'sammans', '##t', '##äll', '##s', 'i', 'en', 'rapport', '.', 'I', 'arbetet', 'kan', 'ingå', 'att', 'förbättra', 'och', 'utveckla', 'nya', 'analys', '##metoder', '.', 'Laborator', '##ie', '##ingenjörer', 'kan', 'även', 'vara', 'med', 'och', 'utarbeta', 'normer', 'och', 'anvisningar', 'för', 'hur', 'man', 'hanterar', 'miljö', '##farliga', 'varor', 'och', 'informera', 'om', 'kemiska', 'och', 'biologiska', 'hälso', '##risker', '.', 'Andra', 'uppgifter', 'kan', 'vara', 'att', 'sköta', 'underhållet', 'av', 'analys', '##apparater', '##na', 'samt', 'att', 'handled', '##a', 'och', 'undervisa', 'den', 'övriga', 'personalen', '.', 'Att', 'söka', 'litteratur', 'och', 'läsa', 'artiklar', 'och', 'rapporter', 'kan', 'också', 'ingå', 'i', 'arbetsuppgifter', '##na', '.', 'På', 'laboratoriet', 'samarbetar', 'ingenjören', 'med', 'flera', 'yrkesk', '##ate', '##gor', '##ier', ',', 'så', 'som', 'labor', '##anter', ',', 'bio', '##medicinska', 'analytiker', ',', 'mikro', '##bio', '##log', '##er', 'och', 'kemist', '##er', '.', 'Arbetsuppgifter', '##na', 'kan', 'vara', 'lika', 'och', 'gränserna', 'för', 'vem', 'som', 'gör', 'vad', 'varierar', 'mellan', 'olika', 'arbetsplatser', '.', '[SEP]']\n",
      "[CLS] Laboratorieingenjör Laboratorieingenjörer kan bland annat arbeta inom den kemiska industrin, massa - och pappersindustrin eller läkemedelsindustrin, men också på forskningsinstitutioner vid högskolor och universitet. Det finns även laboratorieingenjörer vid sjukhus och myndigheter som Livsmedelsverket och Statens Kriminaltekniska Laboratorium. Arbetsuppgifter På laboratoriet arbetar laboratorieingenjören med tekniskt avancerade analysapparater och datorteknik för att göra analyser och kontroller. Man kan undersöka råvaror som används vid tillverkningen på en industri eller prover som tagits ute i produktionen för att kontrollera att produkten håller rätt kvalitet och att tillverkningsprocessen fungerar som den ska. Andra analyser är kemiska och biologiska tester för att undersöka vad som finns i produkter. Det kan till exempel handla om att leta efter ämnen som är skadliga för miljön eller mikroorganismer som kan göra oss sjuka. När en analys är avslutad tolkas svaren och sammanställs i en rapport. I arbetet kan ingå att förbättra och utveckla nya analysmetoder. Laboratorieingenjörer kan även vara med och utarbeta normer och anvisningar för hur man hanterar miljöfarliga varor och informera om kemiska och biologiska hälsorisker. Andra uppgifter kan vara att sköta underhållet av analysapparaterna samt att handleda och undervisa den övriga personalen. Att söka litteratur och läsa artiklar och rapporter kan också ingå i arbetsuppgifterna. På laboratoriet samarbetar ingenjören med flera yrkeskategorier, så som laboranter, biomedicinska analytiker, mikrobiologer och kemister. Arbetsuppgifterna kan vara lika och gränserna för vem som gör vad varierar mellan olika arbetsplatser. [SEP]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('./results/model_CA_5_epochs_weighted_loss.pkl')\n",
    "with open(r'./data/tokenized_data_eval.pkl', \"rb\") as input_file:\n",
    "    test_data = pickle.load(input_file)\n",
    "tokens = tokenizer.convert_ids_to_tokens(test_data[0][\"input_ids\"])\n",
    "print(tokens)\n",
    "dec = tokenizer.decode(test_data[0][\"input_ids\"])\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "44\n",
      "44\n",
      "test idx:  0\n",
      "instance loss:  tensor(1.0268, grad_fn=<NllLossBackward>)\n",
      "Output length:  289\n",
      "labels length:  289\n",
      "test idx:  1\n",
      "instance loss:  tensor(0.9706, grad_fn=<NllLossBackward>)\n",
      "Output length:  315\n",
      "labels length:  315\n",
      "test idx:  2\n",
      "instance loss:  tensor(0.8547, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  3\n",
      "instance loss:  tensor(0.9365, grad_fn=<NllLossBackward>)\n",
      "Output length:  472\n",
      "labels length:  472\n",
      "test idx:  4\n",
      "instance loss:  tensor(0.9099, grad_fn=<NllLossBackward>)\n",
      "Output length:  341\n",
      "labels length:  341\n",
      "test idx:  5\n",
      "instance loss:  tensor(0.9382, grad_fn=<NllLossBackward>)\n",
      "Output length:  332\n",
      "labels length:  332\n",
      "test idx:  6\n",
      "instance loss:  tensor(0.8814, grad_fn=<NllLossBackward>)\n",
      "Output length:  295\n",
      "labels length:  295\n",
      "test idx:  7\n",
      "instance loss:  tensor(1.2728, grad_fn=<NllLossBackward>)\n",
      "Output length:  311\n",
      "labels length:  311\n",
      "test idx:  8\n",
      "instance loss:  tensor(0.6529, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  9\n",
      "instance loss:  tensor(0.9823, grad_fn=<NllLossBackward>)\n",
      "Output length:  333\n",
      "labels length:  333\n",
      "test idx:  10\n",
      "instance loss:  tensor(1.2260, grad_fn=<NllLossBackward>)\n",
      "Output length:  366\n",
      "labels length:  366\n",
      "test idx:  11\n",
      "instance loss:  tensor(0.9214, grad_fn=<NllLossBackward>)\n",
      "Output length:  419\n",
      "labels length:  419\n",
      "test idx:  12\n",
      "instance loss:  tensor(0.4790, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  13\n",
      "instance loss:  tensor(0.8247, grad_fn=<NllLossBackward>)\n",
      "Output length:  405\n",
      "labels length:  405\n",
      "test idx:  14\n",
      "instance loss:  tensor(0.8851, grad_fn=<NllLossBackward>)\n",
      "Output length:  465\n",
      "labels length:  465\n",
      "test idx:  15\n",
      "instance loss:  tensor(0.8240, grad_fn=<NllLossBackward>)\n",
      "Output length:  316\n",
      "labels length:  316\n",
      "test idx:  16\n",
      "instance loss:  tensor(0.3922, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  17\n",
      "instance loss:  tensor(1.1542, grad_fn=<NllLossBackward>)\n",
      "Output length:  362\n",
      "labels length:  362\n",
      "test idx:  18\n",
      "instance loss:  tensor(1.0192, grad_fn=<NllLossBackward>)\n",
      "Output length:  321\n",
      "labels length:  321\n",
      "test idx:  19\n",
      "instance loss:  tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  20\n",
      "instance loss:  tensor(0.8366, grad_fn=<NllLossBackward>)\n",
      "Output length:  407\n",
      "labels length:  407\n",
      "test idx:  21\n",
      "instance loss:  tensor(0.9039, grad_fn=<NllLossBackward>)\n",
      "Output length:  320\n",
      "labels length:  320\n",
      "test idx:  22\n",
      "instance loss:  tensor(0.9265, grad_fn=<NllLossBackward>)\n",
      "Output length:  398\n",
      "labels length:  398\n",
      "test idx:  23\n",
      "instance loss:  tensor(1.1697, grad_fn=<NllLossBackward>)\n",
      "Output length:  406\n",
      "labels length:  406\n",
      "test idx:  24\n",
      "instance loss:  tensor(1.0985, grad_fn=<NllLossBackward>)\n",
      "Output length:  399\n",
      "labels length:  399\n",
      "test idx:  25\n",
      "instance loss:  tensor(1.1116, grad_fn=<NllLossBackward>)\n",
      "Output length:  409\n",
      "labels length:  409\n",
      "test idx:  26\n",
      "instance loss:  tensor(1.0327, grad_fn=<NllLossBackward>)\n",
      "Output length:  447\n",
      "labels length:  447\n",
      "test idx:  27\n",
      "instance loss:  tensor(1.0138, grad_fn=<NllLossBackward>)\n",
      "Output length:  391\n",
      "labels length:  391\n",
      "test idx:  28\n",
      "instance loss:  tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "Output length:  512\n",
      "labels length:  512\n",
      "test idx:  29\n",
      "instance loss:  tensor(0.7948, grad_fn=<NllLossBackward>)\n",
      "Output length:  410\n",
      "labels length:  410\n",
      "test idx:  30\n",
      "instance loss:  tensor(1.2802, grad_fn=<NllLossBackward>)\n",
      "Output length:  412\n",
      "labels length:  412\n",
      "test idx:  31\n",
      "instance loss:  tensor(1.1597, grad_fn=<NllLossBackward>)\n",
      "Output length:  368\n",
      "labels length:  368\n",
      "test idx:  32\n",
      "instance loss:  tensor(1.0596, grad_fn=<NllLossBackward>)\n",
      "Output length:  352\n",
      "labels length:  352\n",
      "test idx:  33\n",
      "instance loss:  tensor(1.0291, grad_fn=<NllLossBackward>)\n",
      "Output length:  389\n",
      "labels length:  389\n",
      "test idx:  34\n",
      "instance loss:  tensor(0.8868, grad_fn=<NllLossBackward>)\n",
      "Output length:  411\n",
      "labels length:  411\n",
      "test idx:  35\n",
      "instance loss:  tensor(0.9662, grad_fn=<NllLossBackward>)\n",
      "Output length:  272\n",
      "labels length:  272\n",
      "test idx:  36\n",
      "instance loss:  tensor(0.7179, grad_fn=<NllLossBackward>)\n",
      "Output length:  298\n",
      "labels length:  298\n",
      "test idx:  37\n",
      "instance loss:  tensor(0.8260, grad_fn=<NllLossBackward>)\n",
      "Output length:  318\n",
      "labels length:  318\n",
      "test idx:  38\n",
      "instance loss:  tensor(0.7684, grad_fn=<NllLossBackward>)\n",
      "Output length:  449\n",
      "labels length:  449\n",
      "test idx:  39\n",
      "instance loss:  tensor(1.0591, grad_fn=<NllLossBackward>)\n",
      "Output length:  360\n",
      "labels length:  360\n",
      "test idx:  40\n",
      "instance loss:  tensor(0.9751, grad_fn=<NllLossBackward>)\n",
      "Output length:  283\n",
      "labels length:  283\n",
      "test idx:  41\n",
      "instance loss:  tensor(0.9009, grad_fn=<NllLossBackward>)\n",
      "Output length:  343\n",
      "labels length:  343\n",
      "test idx:  42\n",
      "instance loss:  tensor(1.0456, grad_fn=<NllLossBackward>)\n",
      "Output length:  388\n",
      "labels length:  388\n",
      "test idx:  43\n",
      "instance loss:  tensor(1.1656, grad_fn=<NllLossBackward>)\n",
      "Output length:  325\n",
      "labels length:  325\n",
      "precision:  0.030215480919706272\n",
      "recall:  0.7382352941176471\n"
     ]
    }
   ],
   "source": [
    "# Output class\n",
    "# https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput\n",
    "\n",
    "model.eval()\n",
    "test_input = []\n",
    "test_labels = []\n",
    "test_attn = []\n",
    "token_type_ids = []\n",
    "for i in range(len(test_data)):\n",
    "    test_input.append(test_data[i]['input_ids'])\n",
    "    test_labels.append(test_data[i]['labels'])\n",
    "    test_attn.append(test_data[i]['attention_mask'])\n",
    "    token_type_ids.append(test_data[i]['token_type_ids'])\n",
    "\n",
    "print(len(test_input))\n",
    "print(len(test_labels))\n",
    "print(len(test_attn))\n",
    "num_correct = 0\n",
    "num_predicted = 0\n",
    "num_pos_data = 0\n",
    "for i in range(len(test_data)):\n",
    "    output = model(torch.tensor([test_data[i]['input_ids']]), attention_mask=torch.tensor([test_data[i]['attention_mask']]), token_type_ids=torch.tensor([test_data[i]['token_type_ids']]), labels=torch.tensor([test_data[i]['labels']]))\n",
    "    print('test idx: ', i)\n",
    "    print('instance loss: ', output.loss)\n",
    "    # print(output.logits)\n",
    "    m = nn.Softmax(dim=2)\n",
    "    max = m(output.logits)\n",
    "    out = torch.argmax(max, dim=2)\n",
    "    # print(max)\n",
    "    print('Output length: ', len(out[0]))\n",
    "    print('labels length: ', len(test_data[i]['labels']))\n",
    "    # print('Labels: ', test_data[i]['labels'])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(test_data[i][\"input_ids\"])\n",
    "    true_labels = test_data[i]['labels']\n",
    "    # print(tokens)\n",
    "    last_label_idx = None\n",
    "    for idx, pred_label in enumerate(out[0]):\n",
    "        true_label = true_labels[idx]\n",
    "        if true_label > 0:\n",
    "            num_pos_data += 1\n",
    "        if pred_label > 0:\n",
    "            # print('label: ', pred_label)\n",
    "            # print('token: ', tokens[idx])\n",
    "            num_predicted += 1\n",
    "            if pred_label == true_label:\n",
    "                num_correct += 1\n",
    "\n",
    "# calculate precision and recall\n",
    "pr = num_correct/num_predicted\n",
    "rec = num_correct/num_pos_data\n",
    "print('precision: ', pr)\n",
    "print('recall: ', rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de4ab37d9aa598fa28430b4c5abb54602406a240d03eddec7af88b85de3986f7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
